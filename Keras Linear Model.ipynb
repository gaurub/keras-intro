{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A brief introduction to Keras \n",
    "\n",
    "We're going to learn a little bit about Keras by creating a model to approximate a linear function. I will walk you through all the steps and my random additions; luckily, in this case, the back prop stuff is all handled by Keras & Tensorflow, so we don't have to worry ourselves with any of that.\n",
    "\n",
    "Keras is a wrapper around Tensorflow, Theano and CNTK; at the moment, there is no Caffe backend. It exposes a very pretty API (compared to TF's crazily complicated API) and makes life a lot easier. It also fits in better with the API signature of scikit-learn, which is a popular framework used for Machine Learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "sess = tf.get_default_session()\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last two lines are important because they tell Keras to use a Tensorflow session that we also have a reference to. This way, we can use Tensorflow and Keras interchangeably and they all work in the same session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "2c7668cf-a67a-4b60-944b-bf4779018b09"
    }
   },
   "source": [
    "#### Premise\n",
    "\n",
    "It's important to remember that neural networks are essentially function approximators; i.e., they learn what a function is given a training set and labels. For example, a network can learn the function that maps images of handwritten digits to labels telling you what those digits are. We first define an architecture and then, using backpropagation along with training data, learn that particular function.\n",
    "\n",
    "In this session, just to get a feel for Keras, we're going to approximate an extremely simple function: a line!\n",
    "\n",
    "In theory, if you can solve this trivial example, you just need to create bigger and better architectures and use different loss functions (according to the use case), along with activation functions in network layers and you'll be able to solve pretty much any problems that a neural network is a good fit for.\n",
    "\n",
    "#### A linear function\n",
    "\n",
    "We're going to define a line as a function that takes in 3 parameters: coefficients $a$ and $b$ and an $x$ coordinate. This function will then map inputs from the domain $X$ to outputs $Y$, thus creating $(X,Y)$ pairs (points) in the Cartesian plane in the following way (I'm just being really elaborate so there's no confusion):\n",
    "\n",
    "$$f(x) = ax + b$$\n",
    "\n",
    "Let's define a linear function that takes in 2 coefficients and returns the point that the function maps it to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true,
    "nbpresent": {
     "id": "43258669-0be2-4de0-b06b-a9f31d4e9348"
    }
   },
   "outputs": [],
   "source": [
    "def f(a,b,x):\n",
    "    return a*x + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "53415e4a-9a85-4130-88fb-9e6d670de1e3"
    }
   },
   "source": [
    "Next, we get our sample from the domain (which will serve as our inputs). In order to make things easy to visualize, I'm going to pull 1000 samples from the interval [-5,5]. The linspace function just creates 1000 (or whatever number you want) samples spaced linearly in an interval. This will serve as our training data, since our function maps these points to others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "154b08f2-c2b1-44a2-85da-08532968eaad"
    }
   },
   "outputs": [],
   "source": [
    "X = np.linspace(-5,5,1000)\n",
    "X = X.reshape((1000,1)) # this line is just to make sure our dimensions play nice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "949d3bf9-59f7-49c2-843a-0723c064da80"
    }
   },
   "source": [
    "Now, we're going to get our Y's, but also add some random noise to make the example just a little more interesting.\n",
    "\n",
    "Our training labels, $Y$, are now defined by the equation $$Y = ax + b + \\epsilon$$ \n",
    "\n",
    "or, using the function $f(x)$,\n",
    "\n",
    "$$Y = f(x) + \\epsilon$$\n",
    "\n",
    "where $\\epsilon$ is random noise sampled from a Gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "1527a9c4-6093-44a0-a4b8-dfe66a240c5f"
    }
   },
   "outputs": [],
   "source": [
    "Y = f(3,2,X) + np.random.randn(X.shape[0], X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "91504c0a-0240-45e1-aad1-4faa11498f8e"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(X,Y, 'bo')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "2fa9d7b6-de3e-4411-b4b6-934456fb4bbf"
    }
   },
   "source": [
    "We now have our input data (in the X vector), where we have 1000 training examples and 1000 training labels in the Y vector. Let's now construct a network that will approximate this. We are going to have 1 neuron with 1 input and 1 output. This neuron will have a weight matrix consisting of one element and a bias vector consisting of one element. As a refresher, we usually write this as:\n",
    "\n",
    "$$ y = Wx + b $$\n",
    "\n",
    "Look familiar?\n",
    "\n",
    "Also, since this is a regression problem (i.e, not a classification problem), we're not going to have an activation in the last layer (which, in this case, also happens to be the first layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true,
    "nbpresent": {
     "id": "3c70c916-0317-4d67-bef6-e54d8ec72b2f"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(1, input_shape=(1,))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're also going to need a loss function; think of all of these things as parts of the network that are swappable based on the use case. For this, a good loss function could be anything that measures distance between points really well; the distance will be measured between the point that the network outputs and the actual label for that particular training sample.\n",
    "\n",
    "Any thoughts on what we could use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "a0ad9ab7-aa4b-4f6f-81e9-bb5262806ff4"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(loss='mse', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have to fit our model to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(X, Y, epochs=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have to see how well our model learned the linear function we were trying to approximate. Since we've picked an architecture that can pretty much only learn a linear function, the results will be a straight line. We're going to evaluate our model on the training set (which is a terrible practice, but shows us how well our line fitting went) and plot those on the original graph to get a sense of how well we did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_predict = model.predict(X)\n",
    "\n",
    "plt.plot(X,Y, 'bo',X,Y_predict,'r-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for kicks, let's also check our weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the weights we got are really close to the coefficients we used for the linear function. We could probably get closer, but that would require more training and as it stands, since we introduced random noise, it might never get to the exact values."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
